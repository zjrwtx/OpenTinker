# Math Training Configuration (GameEnvironment Pattern)
# Use with: python math_client_unified.py

# Project settings
project_name: opentinker
experiment_name: math_training

# Logging
logger_backends: ["console", "wandb"]

# Tracing (optional)
enable_tracing: true
weave_project: null

# WandB (optional)
wandb_key: null

# Model and tokenizer
tokenizer_path: null

# Data paths
data_path: null        # Path to training data (JSON/JSONL)
val_data_path: null    # Path to validation data (JSON/JSONL)

# Training parameters
batch_size: 64
num_workers: 0
# Training duration - set ONE of these (num_steps takes precedence if both set)
num_epochs: 10        # Number of epochs (null = use num_steps)
num_steps: null       # Total training steps (null = use num_epochs)
save_freq: 100
test_freq: 50        # Validation frequency (every N steps)

# Validation parameters
val_batch_size: 100   # Total validation samples

# Model parameters
# Generation parameters
temperature: 0.7
top_p: 1
max_new_tokens: 4098  # TOTAL response budget for entire trajectory
max_prompt_tokens: 4096

# Algorithm - toolcall for math with tool use
algorithm: "agent_loop"


# RL Algorithm settings (passed to server via scheduler)
# adv_estimator options:
#   - "grpo"          : Standard GRPO (outcome-only advantage)
#   - "grpo_per_step" : Per-step GRPO with return-based advantages (for multi-turn tasks)
#   - "gae"           : Generalized Advantage Estimation (for PPO, requires critic)
adv_estimator: "grpo"
# rollout_n: number of samples per prompt for GRPO/grpo_per_step (only used when adv_estimator=grpo or grpo_per_step)
rollout_n: 16

# LoRA configuration (parameter-efficient fine-tuning)
# Set lora_rank > 0 to enable LoRA training
# Reference: verl/verl/trainer/config/model/hf_model.yaml
lora:
  lora_rank: 0         # Set to positive value to enable LoRA (e.g., 64)
  lora_alpha: 16       # LoRA scaling factor
  target_modules: "all-linear"  # Modules to apply LoRA
  exclude_modules: null         # Optional: modules to exclude
  lora_adapter_path: null       # Path to existing adapter for continued training


# Interaction configuration
interaction:
  name: math
  class_path: opentinker.environment.gym_environment_interaction.GymEnvironmentInteraction
  config:
    env_host: 0.0.0.0
    env_port: 8088
    env_endpoint: http://${interaction.config.env_host}:${interaction.config.env_port}
    max_steps: 1     # Max interaction steps

multi_turn:
    max_user_turns: 0
    max_assistant_turns: 1
    max_tokens_per_turn: 4096  # Per-turn response limit
    weave_project: null
    experiment_name: "math_interaction"

# Scheduler settings
scheduler_url: "http://0.0.0.0:8780"
scheduler_api_key: otk_98b8db24ccd64c92e1fdd9a232e209fa

# GPU settings
num_gpus: 4
