# Math Code Interpreter Inference Configuration
# Use with: python math_code_interpreter_inference.py
# This config supports multi-turn code execution for math problem solving

# Scheduler settings
scheduler_url: http://0.0.0.0:8780
scheduler_api_key: null  # Optional API key for authentication

# Model settings
model_path: null           # Path to trained checkpoint (HuggingFace format)
tokenizer_path: null       # Tokenizer path (defaults to model_path if null)

# GPU settings for vLLM server
tensor_parallel_size: 1    # Number of GPUs for tensor parallelism
num_gpus: null             # Override number of GPUs (defaults to tensor_parallel_size)
gpu_memory_utilization: 0.9
max_model_len: null        # Max model context length (optional)
trust_remote_code: true

# Generation parameters (greedy by default for inference)
temperature: 0.0           # 0.0 = greedy decoding
top_p: 1.0
max_new_tokens: 8192       # Total response budget for entire trajectory
max_tokens_per_turn: 1024  # Per-turn response limit

# Data settings
data_path: null            # Input data file (parquet/jsonl)
output_path: null          # Output results file (jsonl)
max_samples: null          # Limit samples (null = all)

# Environment settings (code interpreter math server)
env_endpoint: http://0.0.0.0:8088

# Multi-turn settings (allow code execution iterations)
multi_turn:
    max_user_turns: 5        # Max environment responses (code execution results)
    max_assistant_turns: 5   # Max LLM responses for iterative solving
