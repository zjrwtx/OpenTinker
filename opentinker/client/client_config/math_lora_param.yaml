# Math LoRA Training Configuration
# Use with: python math_rl.py --config-name math_lora_param.yaml
# Based on math_param.yaml with LoRA enabled

# Project settings
project_name: opentinker
experiment_name: math_lora_training

# Logging
logger_backends: ["console", "wandb"]

# Tracing (optional)
enable_tracing: true
weave_project: null

# WandB (optional)
wandb_key: null

# Model and tokenizer
tokenizer_path: null

# Data paths
data_path: null        # Path to training data (JSON/JSONL)
val_data_path: null    # Path to validation data (JSON/JSONL)

# Training parameters
batch_size: 64
num_workers: 0
num_epochs: 10
num_steps: null
save_freq: 100
test_freq: 50

# Validation parameters
val_batch_size: 100

# Generation parameters
temperature: 0.7
top_p: 1
max_new_tokens: 4098
max_prompt_tokens: 4096

# Algorithm
algorithm: "agent_loop"

# RL Algorithm settings
adv_estimator: "grpo"
rollout_n: 16  # Reduced for LoRA to save memory

# LoRA configuration (ENABLED)
# Reference: verl/examples/grpo_trainer/run_qwen2_5-3b_gsm8k_grpo_lora.sh
lora:
  lora_rank: 64        # LoRA rank (higher = more capacity, more memory)
  lora_alpha: 32       # LoRA scaling factor
  target_modules: "all-linear"
  exclude_modules: null
  lora_adapter_path: null
  # Learning rate for LoRA training (higher than full finetune)
  # Recommended: 5e-6 for 1.5B, 3e-6 for 3B, 1e-6 for 7B+
  lr: 5e-6

# Interaction configuration
interaction:
  name: math
  class_path: opentinker.environment.gym_environment_interaction.GymEnvironmentInteraction
  config:
    env_host: 0.0.0.0
    env_port: 8088
    env_endpoint: http://${interaction.config.env_host}:${interaction.config.env_port}
    max_steps: 1

multi_turn:
    max_user_turns: 0
    max_assistant_turns: 1
    max_tokens_per_turn: 4096
    weave_project: null
    experiment_name: "math_lora_interaction"

# Scheduler settings
scheduler_url: "http://0.0.0.0:8780"
scheduler_api_key: otk_98b8db24ccd64c92e1fdd9a232e209fa

# GPU settings (LoRA requires less memory, but 4 GPUs still recommended)
num_gpus: 4
