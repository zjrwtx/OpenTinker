#!/usr/bin/env python3
"""
Example: Launch HTTP PPO Training Server

This script demonstrates how to launch the HTTP training server
that allows clients to send custom training batches via HTTP requests.
"""
import hydra
import os


@hydra.main(config_path="config", config_name="ppo_trainer", version_base=None)
def main(cfg):

    os.environ["WANDB_API_KEY"] = cfg.wandb_key
    os.environ["NCCL_P2P_DISABLE"] = str(cfg.nccl_p2p_disable)
    
    # Only set XFORMERS if not using agent_loop (V1 is incompatible with XFORMERS)
    '''åœ¨server3ä¸Šç¦ç”¨xformers'''
    # if not cfg.get("enable_agent_loop", False):
    #     os.environ["VLLM_ATTENTION_BACKEND"] = "XFORMERS"
    
    os.environ["RAY_DEDUP_LOGS"] = "0"
    os.environ["HYDRA_FULL_ERROR"] = "1"

    import argparse
    import ray
    from omegaconf import OmegaConf
    from omegaconf import open_dict
    import json
    import logging

    from http_training_server import launch_server
    from verl.trainer.ppo.utils import Role, WorkerType

    # Configure logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)


    # ---------------------------------------------------------

    # ðŸ”¥ inline è¦†ç›–æ‰€æœ‰ hydra config å‚æ•°
    with open_dict(cfg):

        # ðŸ”¥ ä»Žå¤–éƒ¨é…ç½®è¯»å– adv_estimatorï¼Œé»˜è®¤ä¸º "grpo"
        # å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è¦†ç›–: algorithm.adv_estimator=ppo
        adv_estimator = cfg.algorithm.get("adv_estimator", "gae")
        cfg.algorithm.adv_estimator = adv_estimator
        logger.info(f"ðŸ”§ Using adv_estimator: {adv_estimator}")

        # comment this
        cfg.data.max_prompt_length = 1024
        cfg.data.max_response_length = 1024

        # actor-rollout-ref
        # cfg.actor_rollout_ref.model.path = cfg.model_path
        # Use CLI-provided lr if available, otherwise default to 1e-6
        if cfg.actor_rollout_ref.actor.optim.get("lr") is None:
            cfg.actor_rollout_ref.actor.optim.lr = 1e-6
        cfg.actor_rollout_ref.model.use_remove_padding = True
        cfg.actor_rollout_ref.model.enable_gradient_checkpointing = True
        cfg.actor_rollout_ref.actor.ppo_mini_batch_size = 16
        cfg.actor_rollout_ref.actor.use_dynamic_bsz = True
        cfg.actor_rollout_ref.actor.fsdp_config.param_offload = False
        cfg.actor_rollout_ref.actor.fsdp_config.optimizer_offload = False
        
        # PPO é»˜è®¤è®¾ç½®
        cfg.actor_rollout_ref.actor.use_kl_loss = False # False for PPO, True for GRPO
        cfg.algorithm.use_kl_in_reward = True # True for PPO, False for GRPO

        cfg.actor_rollout_ref.rollout.tensor_model_parallel_size = 2
        cfg.actor_rollout_ref.rollout.name = "vllm"
        cfg.actor_rollout_ref.rollout.gpu_memory_utilization = 0.6

        # GRPO/GRPO-per-step ç‰¹å®šé…ç½®
        # grpo_per_step uses the same training framework as grpo, just with different advantage estimation
        if cfg.algorithm.adv_estimator in ("grpo", "grpo_per_step"):
            # ä»Žå¤–éƒ¨é…ç½®è¯»å– rollout.nï¼Œé»˜è®¤ä¸º 4
            # å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è¦†ç›–: actor_rollout_ref.rollout.n=8
            rollout_n = cfg.actor_rollout_ref.rollout.get("n", 4)
            cfg.actor_rollout_ref.rollout.n = rollout_n
            logger.info(f"ðŸ”§ {adv_estimator} mode: rollout.n = {rollout_n}")
            
            cfg.actor_rollout_ref.actor.kl_loss_coef = 0.001
            cfg.actor_rollout_ref.actor.kl_loss_type = "low_var_kl"
            cfg.actor_rollout_ref.actor.use_kl_loss = True
            cfg.algorithm.use_kl_in_reward = False  # GRPO ä½¿ç”¨ False
        else:
            # PPO/GAE æ¨¡å¼ï¼šæ˜¾å¼è®¾ç½® rollout.n = 1ï¼ˆå¿½ç•¥ä»»ä½•å‘½ä»¤è¡Œä¼ å…¥çš„å€¼ï¼‰
            cfg.actor_rollout_ref.rollout.n = 1
            logger.info(f"ðŸ”§ PPO/GAE mode: rollout.n = 1 (forced)")

        # LoRA-specific configuration
        # LoRA params are passed via CLI from scheduler (lora_rank, lora_alpha, etc.)
        lora_rank = cfg.actor_rollout_ref.model.get("lora_rank", 0)
        if lora_rank > 0:
            logger.info(f"ðŸ”§ LoRA mode enabled: rank={lora_rank}")
            
            # Log current lr (may be set via CLI from client yaml)
            current_lr = cfg.actor_rollout_ref.actor.optim.get("lr", 5e-6)
            logger.info(f"  - lr: {current_lr}")
            
            # Entropy coefficient: 0 for stable LoRA training
            cfg.actor_rollout_ref.actor.entropy_coeff = 0
            
            # Enable layered summon for memory efficiency with LoRA
            cfg.actor_rollout_ref.rollout.layered_summon = True
            # Use safetensors format for LoRA adapter loading
            cfg.actor_rollout_ref.rollout.load_format = "safetensors"
            logger.info(f"  - layered_summon: True, load_format: safetensors")


        # critic
        cfg.critic.optim.lr = 1e-5
        cfg.critic.model.use_remove_padding = True
        # cfg.critic.model.path = cfg.model_path
        cfg.critic.model.enable_gradient_checkpointing = True
        # Enable offloading to reduce memory usage during initialization
        cfg.critic.model.fsdp_config.param_offload = False
        cfg.critic.model.fsdp_config.optimizer_offload = False

        # trainer
        cfg.trainer.critic_warmup = 0
        cfg.trainer.logger = ["console", "wandb"]
        cfg.trainer.project_name = "OpenTinker"
        cfg.trainer.experiment_name = "qwen2.5-3b"
        cfg.trainer.n_gpus_per_node = 4
        cfg.trainer.val_before_train = True
        cfg.trainer.nnodes = 1
        cfg.trainer.save_freq = 500
        cfg.trainer.test_freq = 500
        cfg.trainer.total_epochs = 15
        cfg.trainer.default_local_dir = "/workspace/verl/verl/ckpts"

    # ---------------------------------------------------------
    # Agent Loop Configuration
    # Parse command-line arguments for agent_loop mode

    # Sandbox will be created after Ray initialization to avoid being destroyed by ray.shutdown()
    sandbox_actor = None
    sandbox_address = None
    
    if cfg.enable_agent_loop:
        logger.info("=" * 60)
        logger.info("Agent Loop Mode Enabled")
        logger.info("=" * 60)
        
        # Set VLLM_USE_V1 for async mode
        os.environ["VLLM_USE_V1"] = "1"
        logger.info("Set VLLM_USE_V1=1 for async rollout")
        
        # Increase Ray's memory threshold to avoid premature OOM kills
        # Default is 0.95 (95%), we increase to 0.98 (98%)
        os.environ["RAY_memory_usage_threshold"] = "0.98"
        logger.info("Set RAY_memory_usage_threshold=0.98 to allow higher memory usage")

        # DON'T create Sandbox here - it will be created after Ray initialization
        # to avoid being destroyed by ray.shutdown() below

        # Configure multi-turn settings in hydra config
        with open_dict(cfg):
            cfg.actor_rollout_ref.rollout.mode = "async"
            cfg.actor_rollout_ref.rollout.multi_turn.enable = True
            # Only set defaults if not already configured (allow client to override)
            if cfg.actor_rollout_ref.rollout.multi_turn.get("max_assistant_turns", None) is None:
                cfg.actor_rollout_ref.rollout.multi_turn.max_assistant_turns = 10
            if cfg.actor_rollout_ref.rollout.multi_turn.get("max_user_turns", None) is None:
                cfg.actor_rollout_ref.rollout.multi_turn.max_user_turns = 10
            cfg.actor_rollout_ref.rollout.multi_turn.format = "hermes"
            cfg.actor_rollout_ref.rollout.multi_turn.max_parallel_calls = 1
            cfg.actor_rollout_ref.rollout.multi_turn.max_tool_response_length = 2000
            cfg.actor_rollout_ref.rollout.multi_turn.tool_response_truncate_side = "right"
            # cfg.actor_rollout_ref.rollout.agent.default_agent_loop = "generic_agent"
            cfg.actor_rollout_ref.rollout.agent.agent_loop_config_path = "opentinker/server/agent.yaml"
            cfg.actor_rollout_ref.rollout.agent.num_workers = 8
            cfg.data.return_raw_chat = True  # Required for agent_loop
            
            # Disable thinking mode for agent_loop
            if not hasattr(cfg.data, "apply_chat_template_kwargs"):
                cfg.data.apply_chat_template_kwargs = {}
            cfg.data.apply_chat_template_kwargs.enable_thinking = False

        logger.info("âœ“ Agent loop configuration applied (Sandbox will be created after Ray init)")
        logger.info(f"  - Rollout mode: {cfg.actor_rollout_ref.rollout.mode}")
        logger.info(f"  - Multi-turn enabled: {cfg.actor_rollout_ref.rollout.multi_turn.enable}")
        logger.info(f"  - Default agent loop: {cfg.actor_rollout_ref.rollout.agent.default_agent_loop}")
        logger.info(f"  - return_raw_chat: {cfg.data.return_raw_chat}")
        logger.info("=" * 60)
    # ---------------------------------------------------------

    # Pass job_id to server if provided by scheduler
    job_id = cfg.get("job_id", None)
    if job_id:
        logger.info(f"Job ID: {job_id} (set by scheduler for resource tracking)")

    launch_server(cfg, job_id=job_id)



if __name__ == "__main__":
    main()
